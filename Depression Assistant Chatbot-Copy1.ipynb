{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Collecting protobuf>=3.19.6\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "\u001b[K     |████████████████████████████████| 414 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tf-keras>=2.14.1\n",
      "  Downloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (1.20.1)\n",
      "  Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tf-keras, protobuf, tensorflow-hub\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "Successfully installed protobuf-5.28.3 tensorflow-hub-0.16.1 tf-keras-2.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/kritika/.local/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kritika/.local/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kritika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/kritika/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['how', 'the', 'fuk', 'who', 'the', 'heck', 'moved', 'my', 'fridge', 'should', 'i', 'knock', 'the', 'landlord', 'door', 'angry', 'mad'], 'anger')\n",
      "760\n",
      "995\n",
      "673\n",
      "714\n",
      "4689\n",
      "0\n",
      "['anger', 'fear', 'sadness', 'joy']\n",
      "(3751, 7580)\n",
      "(3751, 4)\n",
      "(938, 7580)\n",
      "(938, 4)\n",
      " Shape of X is  (7580, 3751)\n",
      " Shape of Y is  (4, 3751)\n",
      " Shape of m is  3751\n",
      " Shape of W1 is  (100, 7580)\n",
      " Shape of W2 is  (4, 100)\n",
      "################### TRAIN MODEL STATISTICS ######################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.16      0.21      1766\n",
      "           1       0.22      0.29      0.26       824\n",
      "           2       0.15      0.24      0.18       534\n",
      "           3       0.25      0.35      0.29       627\n",
      "\n",
      "    accuracy                           0.23      3751\n",
      "   macro avg       0.23      0.26      0.23      3751\n",
      "weighted avg       0.25      0.23      0.23      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.77      0.50       443\n",
      "           1       0.95      0.49      0.65      2077\n",
      "           2       0.50      0.81      0.62       529\n",
      "           3       0.64      0.81      0.72       702\n",
      "\n",
      "    accuracy                           0.63      3751\n",
      "   macro avg       0.61      0.72      0.62      3751\n",
      "weighted avg       0.76      0.63      0.64      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       918\n",
      "           1       0.86      0.88      0.87      1057\n",
      "           2       0.78      0.83      0.80       807\n",
      "           3       0.87      0.80      0.84       969\n",
      "\n",
      "    accuracy                           0.82      3751\n",
      "   macro avg       0.82      0.82      0.82      3751\n",
      "weighted avg       0.83      0.82      0.82      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       800\n",
      "           1       0.94      0.75      0.83      1358\n",
      "           2       0.75      0.90      0.82       712\n",
      "           3       0.87      0.88      0.87       881\n",
      "\n",
      "    accuracy                           0.83      3751\n",
      "   macro avg       0.83      0.85      0.83      3751\n",
      "weighted avg       0.85      0.83      0.83      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.77      0.82      1050\n",
      "           1       0.79      0.99      0.88       858\n",
      "           2       0.84      0.82      0.83       886\n",
      "           3       0.90      0.84      0.87       957\n",
      "\n",
      "    accuracy                           0.85      3751\n",
      "   macro avg       0.85      0.86      0.85      3751\n",
      "weighted avg       0.86      0.85      0.85      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83       698\n",
      "           1       0.98      0.68      0.81      1551\n",
      "           2       0.75      0.95      0.84       674\n",
      "           3       0.86      0.93      0.89       828\n",
      "\n",
      "    accuracy                           0.84      3751\n",
      "   macro avg       0.83      0.88      0.84      3751\n",
      "weighted avg       0.87      0.84      0.84      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83      1220\n",
      "           1       0.76      1.00      0.87       827\n",
      "           2       0.84      0.88      0.86       825\n",
      "           3       0.90      0.91      0.90       879\n",
      "\n",
      "    accuracy                           0.86      3751\n",
      "   macro avg       0.87      0.88      0.87      3751\n",
      "weighted avg       0.88      0.86      0.86      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.98      0.83       668\n",
      "           1       0.98      0.71      0.82      1499\n",
      "           2       0.77      0.95      0.85       698\n",
      "           3       0.90      0.91      0.91       886\n",
      "\n",
      "    accuracy                           0.85      3751\n",
      "   macro avg       0.84      0.89      0.85      3751\n",
      "weighted avg       0.88      0.85      0.85      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.71      0.83      1278\n",
      "           1       0.78      1.00      0.88       849\n",
      "           2       0.84      0.92      0.88       787\n",
      "           3       0.89      0.95      0.92       837\n",
      "\n",
      "    accuracy                           0.87      3751\n",
      "   macro avg       0.88      0.90      0.88      3751\n",
      "weighted avg       0.89      0.87      0.87      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86       713\n",
      "           1       0.97      0.83      0.89      1272\n",
      "           2       0.85      0.92      0.88       800\n",
      "           3       0.95      0.88      0.91       966\n",
      "\n",
      "    accuracy                           0.89      3751\n",
      "   macro avg       0.89      0.90      0.89      3751\n",
      "weighted avg       0.90      0.89      0.89      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87      1172\n",
      "           1       0.86      0.99      0.92       939\n",
      "           2       0.87      0.93      0.90       801\n",
      "           3       0.90      0.96      0.93       839\n",
      "\n",
      "    accuracy                           0.91      3751\n",
      "   macro avg       0.91      0.92      0.91      3751\n",
      "weighted avg       0.91      0.91      0.90      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91       820\n",
      "           1       0.97      0.91      0.94      1149\n",
      "           2       0.89      0.92      0.91       831\n",
      "           3       0.96      0.90      0.93       951\n",
      "\n",
      "    accuracy                           0.92      3751\n",
      "   macro avg       0.92      0.93      0.92      3751\n",
      "weighted avg       0.93      0.92      0.92      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.91      1069\n",
      "           1       0.92      0.98      0.95      1012\n",
      "           2       0.89      0.94      0.91       809\n",
      "           3       0.93      0.97      0.95       861\n",
      "\n",
      "    accuracy                           0.93      3751\n",
      "   macro avg       0.93      0.93      0.93      3751\n",
      "weighted avg       0.93      0.93      0.93      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       899\n",
      "           1       0.96      0.95      0.96      1099\n",
      "           2       0.91      0.94      0.93       830\n",
      "           3       0.97      0.94      0.95       923\n",
      "\n",
      "    accuracy                           0.94      3751\n",
      "   macro avg       0.94      0.94      0.94      3751\n",
      "weighted avg       0.94      0.94      0.94      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.94      1013\n",
      "           1       0.95      0.98      0.96      1045\n",
      "           2       0.90      0.95      0.93       811\n",
      "           3       0.96      0.97      0.96       882\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       943\n",
      "           1       0.96      0.97      0.97      1073\n",
      "           2       0.92      0.95      0.94       828\n",
      "           3       0.97      0.95      0.96       907\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       975\n",
      "           1       0.96      0.97      0.97      1065\n",
      "           2       0.92      0.96      0.94       821\n",
      "           3       0.97      0.97      0.97       890\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       956\n",
      "           1       0.96      0.97      0.97      1071\n",
      "           2       0.93      0.96      0.94       830\n",
      "           3       0.97      0.97      0.97       894\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       964\n",
      "           1       0.96      0.97      0.97      1070\n",
      "           2       0.93      0.96      0.94       827\n",
      "           3       0.97      0.97      0.97       890\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       958\n",
      "           1       0.97      0.97      0.97      1078\n",
      "           2       0.93      0.96      0.95       823\n",
      "           3       0.97      0.97      0.97       892\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       960\n",
      "           1       0.97      0.98      0.97      1073\n",
      "           2       0.93      0.96      0.95       825\n",
      "           3       0.98      0.98      0.98       893\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       960\n",
      "           1       0.97      0.98      0.97      1073\n",
      "           2       0.93      0.96      0.95       825\n",
      "           3       0.98      0.98      0.98       893\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       959\n",
      "           1       0.97      0.98      0.97      1073\n",
      "           2       0.93      0.97      0.95       825\n",
      "           3       0.98      0.98      0.98       894\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       961\n",
      "           1       0.97      0.98      0.98      1071\n",
      "           2       0.93      0.97      0.95       828\n",
      "           3       0.98      0.98      0.98       891\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       955\n",
      "           1       0.97      0.98      0.98      1071\n",
      "           2       0.94      0.97      0.96       834\n",
      "           3       0.98      0.98      0.98       891\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       954\n",
      "           1       0.97      0.98      0.98      1070\n",
      "           2       0.95      0.97      0.96       837\n",
      "           3       0.98      0.98      0.98       890\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       954\n",
      "           1       0.97      0.99      0.98      1068\n",
      "           2       0.95      0.97      0.96       838\n",
      "           3       0.98      0.98      0.98       891\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       953\n",
      "           1       0.98      0.99      0.98      1070\n",
      "           2       0.95      0.97      0.96       837\n",
      "           3       0.98      0.98      0.98       891\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       949\n",
      "           1       0.98      0.99      0.98      1071\n",
      "           2       0.95      0.97      0.96       839\n",
      "           3       0.98      0.98      0.98       892\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       949\n",
      "           1       0.98      0.99      0.98      1070\n",
      "           2       0.95      0.97      0.96       840\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       945\n",
      "           1       0.98      0.99      0.98      1073\n",
      "           2       0.96      0.97      0.96       841\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       947\n",
      "           1       0.98      0.99      0.98      1071\n",
      "           2       0.96      0.98      0.97       841\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       945\n",
      "           1       0.98      0.99      0.98      1072\n",
      "           2       0.96      0.98      0.97       842\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       945\n",
      "           1       0.98      0.99      0.98      1071\n",
      "           2       0.96      0.98      0.97       843\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       943\n",
      "           1       0.98      0.99      0.99      1073\n",
      "           2       0.96      0.98      0.97       842\n",
      "           3       0.99      0.99      0.99       893\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       941\n",
      "           1       0.98      0.99      0.99      1073\n",
      "           2       0.96      0.98      0.97       844\n",
      "           3       0.99      0.99      0.99       893\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       941\n",
      "           1       0.98      0.99      0.99      1075\n",
      "           2       0.96      0.98      0.97       844\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       939\n",
      "           1       0.98      0.99      0.99      1076\n",
      "           2       0.96      0.98      0.97       846\n",
      "           3       0.99      0.99      0.99       890\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       938\n",
      "           1       0.99      0.99      0.99      1076\n",
      "           2       0.96      0.98      0.97       846\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       938\n",
      "           1       0.99      0.99      0.99      1075\n",
      "           2       0.97      0.98      0.97       847\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       938\n",
      "           1       0.99      0.99      0.99      1075\n",
      "           2       0.97      0.98      0.97       847\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       938\n",
      "           1       0.99      0.99      0.99      1075\n",
      "           2       0.97      0.98      0.97       847\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       937\n",
      "           1       0.99      0.99      0.99      1074\n",
      "           2       0.97      0.98      0.97       849\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.98      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       936\n",
      "           1       0.99      0.99      0.99      1074\n",
      "           2       0.97      0.98      0.97       849\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       935\n",
      "           1       0.99      0.99      0.99      1074\n",
      "           2       0.97      0.98      0.97       850\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       933\n",
      "           1       0.99      0.99      0.99      1075\n",
      "           2       0.97      0.98      0.97       850\n",
      "           3       0.99      0.99      0.99       893\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       932\n",
      "           1       0.99      0.99      0.99      1076\n",
      "           2       0.97      0.98      0.98       851\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       932\n",
      "           1       0.99      0.99      0.99      1076\n",
      "           2       0.97      0.98      0.98       851\n",
      "           3       0.99      0.99      0.99       892\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       931\n",
      "           1       0.99      0.99      0.99      1076\n",
      "           2       0.97      0.98      0.98       853\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       930\n",
      "           1       0.99      0.99      0.99      1077\n",
      "           2       0.97      0.98      0.98       853\n",
      "           3       0.99      0.99      0.99       891\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "saved synapses to: weights.json\n",
      " Shape of X is  (7580, 938)\n",
      " Shape of Y is  (4, 938)\n",
      " Shape of m is  938\n",
      "################### TEST MODEL STATISTICS ######################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93       262\n",
      "           1       0.92      0.94      0.93       253\n",
      "           2       0.90      0.89      0.89       202\n",
      "           3       0.94      0.96      0.95       221\n",
      "\n",
      "    accuracy                           0.93       938\n",
      "   macro avg       0.92      0.92      0.92       938\n",
      "weighted avg       0.93      0.93      0.93       938\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "anger_training_set = []\n",
    "fear_training_set = []\n",
    "sadness_training_set = []\n",
    "joy_training_set = []\n",
    "\n",
    "anger_test_set = []\n",
    "fear_test_set = []\n",
    "sadness_test_set = []\n",
    "joy_test_set = []\n",
    "stemmer = LancasterStemmer()\n",
    "all_words=[]\n",
    "\n",
    "# Here I am loading the dataset from stored folder. The training data is stored as text file and each tweet is accompanied\n",
    "# by the magnitude of its sentiment (0 to 1). I had to go through the tweets myself and observed that a threshold of 0.5 is \n",
    "# good enough to classify a tweet according to its sentiment. Tweets with lesser threshold were not definitive to be trained as per their mentioned classification  \n",
    "# I only read those tweets that have a dominant classification factor i.e. above 0.5\n",
    "# Here i am setting each tweet's threshold magnitude accordingly\n",
    "def load_training_data(sentiment):\n",
    "    data = open(f'{sentiment}_training_set.txt',encoding=\"utf8\")\n",
    "    if sentiment == \"anger\":        \n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"fear\":\n",
    "        threshold = 0.6\n",
    "    elif sentiment == \"sadness\":\n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"joy\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        pass\n",
    "    return data,threshold\n",
    "\n",
    "\n",
    "def load_test_data(sentiment):\n",
    "    data = open(f'{sentiment}_test_set.txt',encoding=\"utf8\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# In this method, I am cleaning the tweet data removing punctuations and then tokenizing the words in tweet removing name tags\n",
    "# and appending them to training set\n",
    "def clean_data(training_data,threshold):\n",
    "    training_set = []\n",
    "    for line in training_data:\n",
    "        line = line.strip().lower()\n",
    "        if line.split()[-1] == \"none\":\n",
    "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "            result = line.translate(punct)\n",
    "            tokened_sentence = nltk.word_tokenize(result)\n",
    "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "            label = tokened_sentence[-2]\n",
    "            training_set.append((sentence,label))\n",
    "        else:\n",
    "            intensity = float(line.split()[-1])        \n",
    "            if (intensity>=threshold):\n",
    "                line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "                punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "                result = line.translate(punct)\n",
    "                tokened_sentence = nltk.word_tokenize(result)\n",
    "                sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "                label = tokened_sentence[-1]\n",
    "                training_set.append((sentence,label))\n",
    "    return training_set\n",
    "    \n",
    "# This method collects all the unique words that are contained in the entire tweet dataset, finds their stem and \n",
    "# encodes each sentence according to the bag of words appending it to training set\n",
    "def bag_of_words(all_data):\n",
    "    training_set = []\n",
    "    all_words = []\n",
    "    for each_list in all_data:\n",
    "        for words in each_list[0]:\n",
    "            word = stemmer.stem(words)\n",
    "            all_words.append(word)\n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    for each_sentence in all_data:  \n",
    "        bag = [0]*len(all_words)\n",
    "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
    "    return training_set,all_words\n",
    "\n",
    "# Here we encode each tweet's words according to the words it contained from the bag of words which is based on all words in all tweets\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    bag = [] \n",
    "    all_data = []\n",
    "    all_test_data = []\n",
    "    labels = []\n",
    "    classes = []\n",
    "    labels = []\n",
    "    test_labels = []\n",
    "    words=[]\n",
    "    test_words = []\n",
    "        \n",
    "    ######### Here we read the whole training data for each class and the threshold we will use for its classification\n",
    "    anger_training_data,threshold = load_training_data(\"anger\")\n",
    "    anger_training_set = clean_data(anger_training_data,threshold)\n",
    "    print(anger_training_set[0])\n",
    "    \n",
    "    fear_training_data,threshold = load_training_data(\"fear\")\n",
    "    fear_training_set = clean_data(fear_training_data,threshold)\n",
    "    \n",
    "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
    "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
    "    \n",
    "    joy_training_data,threshold = load_training_data(\"joy\")\n",
    "    joy_training_set = clean_data(joy_training_data,threshold)\n",
    "    \n",
    "    \n",
    "    ######### Here we read the whole test data for each class and the threshold we will use for its classification\n",
    "    anger_test_data = load_test_data(\"anger\")\n",
    "    anger_test_set = clean_data(anger_test_data,threshold)\n",
    "    #print(anger_test_set[0])\n",
    "    print(len(anger_test_set))\n",
    "    \n",
    "    fear_test_data = load_test_data(\"fear\")\n",
    "    fear_test_set = clean_data(fear_test_data,threshold)\n",
    "   # print(fear_test_set[0])\n",
    "    print(len(fear_test_set))\n",
    "    \n",
    "    sadness_test_data = load_test_data(\"sadness\")\n",
    "    sadness_test_set = clean_data(sadness_test_data,threshold)\n",
    "  #  print(sadness_test_set[0])\n",
    "    print(len(sadness_test_set))\n",
    "    \n",
    "    joy_test_data = load_test_data(\"joy\")\n",
    "    joy_test_set = clean_data(joy_test_data,threshold)\n",
    "  #  print(joy_test_set[0])\n",
    "    print(len(joy_test_set))\n",
    "    ###### In every training set above we have a nested list whose first element is sentence and 2nd element its respective label ######\n",
    "    \n",
    "#    print(anger_training_set[0][0],anger_training_set[0][1])\n",
    "#    print(joy_training_set[0][0],joy_training_set[0][1])\n",
    "\n",
    "    \n",
    "    ###### Here we combine all training sets in one list ######\n",
    "    all_data.extend(anger_training_set)\n",
    "    all_data.extend(fear_training_set)\n",
    "    all_data.extend(sadness_training_set)\n",
    "    all_data.extend(joy_training_set)\n",
    "    \n",
    "    all_data.extend(anger_test_set)\n",
    "    all_data.extend(fear_test_set)\n",
    "    all_data.extend(sadness_test_set)\n",
    "    all_data.extend(joy_test_set)\n",
    "    \n",
    "    ###### Here we simply make a classification label list encoding our 4 classes as follows\n",
    "    \n",
    "    \n",
    "    for i,j in all_data:\n",
    "        if j == \"anger\":            \n",
    "            labels.append([1,0,0,0])\n",
    "        elif j == \"fear\":            \n",
    "            labels.append([0,1,0,0])\n",
    "        elif j == \"sadness\":            \n",
    "            labels.append([0,0,1,0])\n",
    "        elif j == \"joy\":            \n",
    "            labels.append([0,0,0,1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print(len(labels))\n",
    "    print(len(test_labels))\n",
    "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
    "    print(classes)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    \n",
    "    # Here we will have the whole training set and the all the words contained in whole training set\n",
    "    training_set,words = bag_of_words(all_data)\n",
    "    \n",
    "    # We convert our training,test set and training, test labels in a numpy array as it is required for calculations in neural net\n",
    "    dataset = np.array(training_set)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # It is important to shuffle dataset so your classifier does not attempt to memorize training set, this functions shuffles data and labels.\n",
    "    shuffling_function = np.random.permutation(dataset.shape[0])\n",
    "    shuffled_dataset, shuffled_labels = np.zeros((dataset.shape)),np.zeros((dataset.shape))\n",
    "    shuffled_dataset,shuffled_labels = dataset[shuffling_function],labels[shuffling_function]\n",
    "    \n",
    "    \n",
    "    split = int(len(shuffled_dataset)*0.8)\n",
    "    training_data = shuffled_dataset[:split]\n",
    "    training_labels = shuffled_labels[:split]\n",
    "    test_data = shuffled_dataset[split:]\n",
    "    test_labels = shuffled_labels[split:]\n",
    "    print(training_data.shape)\n",
    "    print(training_labels.shape)    \n",
    "    print(test_data.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "        \n",
    "    ############# HERE WE HAVE A SHUFFLED DATASET WITH RESPECTIVE LABELS NOW WE HAVE TO TRAIN THIS DATA BOTH NUMPY ARRAYS ############\n",
    "    Train_model(training_data,training_labels,words,classes)\n",
    "    Test_model(test_data,test_labels,words,classes)\n",
    "\n",
    "# Method for calculating sigmoid\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "# Method for calculating forward propagation\n",
    "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "# Method for calculating relu activation's derivative\n",
    "def relu_backward(da,dz):\n",
    "    da1 = np.array(da,copy=True)\n",
    "    da1[dz<0]=0\n",
    "    assert da1.shape == dz.shape\n",
    "    return da1\n",
    "\n",
    "# Method for calculating linear part of backward propagation\n",
    "def linear_backward(dz,a,m,w,b):\n",
    "    dw = (1/m)*np.dot(dz,a.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "    da = np.dot(w.T,dz)\n",
    "    assert (dw.shape==w.shape)\n",
    "    assert (da.shape==a.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return da,dw,db \n",
    "\n",
    "# Method for calculating loss function\n",
    "def calculate_loss(Y,Yhat,m):\n",
    "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
    "    return loss\n",
    "\n",
    "# Method for back propagation\n",
    "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
    "    dZ2 = A2-Y\n",
    "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
    "    dZ1 = relu_backward(da1,Z1)\n",
    "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
    "    W2 = W2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Test_model(test_data, test_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = test_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = test_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    \n",
    "    weights_file = 'weights.json' \n",
    "    with open(weights_file) as data_file: \n",
    "        weights = json.load(data_file) \n",
    "        W1 = np.asarray(weights['weight1']) \n",
    "        W2 = np.asarray(weights['weight2'])\n",
    "        b1 = np.asarray(weights['bias1']) \n",
    "        b2 = np.asarray(weights['bias2'])\n",
    "\n",
    "    print(\"################### TEST MODEL STATISTICS ######################\")\n",
    "    for i in range(1):\n",
    "        # input layer is our encoded sentence\n",
    "        l0 = X\n",
    "        # matrix multiplication of input and hidden layer\n",
    "        l1 = relu(np.dot(W1,l0)+b1)\n",
    "        # output layer\n",
    "        l2 = softmax(np.dot(W2,l1)+b2)\n",
    "        predictions = np.argmax(l2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Train_model(training_data, training_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = training_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = training_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    # Multiplying by 0.01 so that we get smaller weights .. dimensions 100x3787\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    print(\" Shape of W1 is \", W1.shape)\n",
    "    # Dimensions 100x1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    # Dimensions 1547 x 4\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    print(\" Shape of W2 is \", W2.shape)\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    print(\"################### TRAIN MODEL STATISTICS ######################\")\n",
    "    for i in range(0,iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "        Loss = calculate_loss(Y,A2,m)\n",
    "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
    "        all_losses.append(Loss)\n",
    "\n",
    "    # storing weights so that we can reuse them without having to retrain the neural network\n",
    "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(), \n",
    "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", weights_file)\n",
    "    plt.plot(all_losses)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model((\"senti.h5\"),custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.1\n",
    "# load our calculated weight values\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    W1 = np.asarray(weights['weight1']) \n",
    "    W2 = np.asarray(weights['weight2'])\n",
    "    b1 = np.asarray(weights['bias1']) \n",
    "    b2 = np.asarray(weights['bias2'])\n",
    "    all_words = weights['words']\n",
    "    classes = weights['classes']\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "def clean_sentence(verification_data):\n",
    "    line = verification_data\n",
    "    # Remove whitespace from line and lower case iter\n",
    "    line = line.strip().lower()\n",
    "    # Removing word with @ sign as we dont need name tags of twitter\n",
    "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "    # Remove punctuations and numbers from the line\n",
    "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "    result = line.translate(punct)\n",
    "    # Tokenize the whole tweet sentence\n",
    "    tokened_sentence = nltk.word_tokenize(result)\n",
    "    # We take the tweet sentence from tokened sentence\n",
    "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
    "    return sentence    \n",
    "\n",
    "def verify(sentence, show_details=False):\n",
    "    bag=[0]*len(all_words)\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    # This line returns the bag of words as 0 or 1 if words in sentence are found in all_words\n",
    "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape(x.shape[0],1)\n",
    "    \n",
    "#    print(\"Shape of X is \", x.shape)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our encoded sentence\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = relu(np.dot(W1,l0)+b1)\n",
    "    # output layer\n",
    "    l2 = softmax(np.dot(W2,l1)+b2)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = verify(sentence, show_details)\n",
    "    result = np.array(results)\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    return return_results[0][0], result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! what's up? I broke up with my boyfriend. I feel like dying.\n",
      "You seem to be feeling anger\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "input_sentiment = input(\"Hey! What's up? \")\n",
    "sentiment, result= classify(input_sentiment)\n",
    "check = model.predict(np.expand_dims(input_sentiment,0))\n",
    "print(f'You seem to be feeling {sentiment}')\n",
    "if sentiment == \"anger\" or sentiment == \"sadness\" or sentiment == \"fear\":\n",
    "    answer = input(\"Sorry to hear that .... would you like to hear a joke to lighten your mood ? Press Yes or No \")\n",
    "    if answer == \"N\" or answer == \"No\" or answer == \"no\" or answer == \"n\":\n",
    "        print(\"Have a nice day. Goodbye :) \")\n",
    "    else:\n",
    "        file = open('jokes.txt','r',encoding=\"cp1252\")\n",
    "        while(1):\n",
    "            full_file = file.readline()\n",
    "            split_file = full_file.split('/')\n",
    "        #    print(split_file)\n",
    "            slashes = full_file.count('/')\n",
    "        #    print(slashes)\n",
    "            line_of_joke = []\n",
    "            for i in range(slashes):\n",
    "                k=0\n",
    "            #    print(split_file[i])\n",
    "                commas = split_file[i].count('\"')\n",
    "        #        print(commas)\n",
    "                length = int(commas/2)\n",
    "                if length == 0:\n",
    "                    line_of_joke.append(split_file[i])\n",
    "                else:\n",
    "                    for j in range(length):\n",
    "        #            print(\"Here\")\n",
    "                        line_of_joke.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
    "                        if j==length-1:\n",
    "                            line_of_joke.append(split_file[i].split('\"')[k+2])\n",
    "                        k=k+2\n",
    "    #    break\n",
    "            for i in line_of_joke:\n",
    "                print(i)\n",
    "            user_input = input(\"Do you want another joke ? Write Yes or No\\t\")\n",
    "            if user_input in ['Y','y','Yes','yes','YES']:\n",
    "                clear_output()\n",
    "                pass\n",
    "            else:\n",
    "                clear_output()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(\"i want to die\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bank=['How are you', \n",
    "'Tell me something',\n",
    "'How was your day today',\n",
    "'Do you like people',\n",
    "'What are your hobbies',\n",
    "'Tell me about your personality',\n",
    "'Tell me about your parents',\n",
    "'Do you have any friends and tell about your best friend',\n",
    "'Are you currently facing any problem and tell about that problem',\n",
    "'Describe your recent bullying incident',\n",
    "'What are the things you afraid most',\n",
    "'What are you thinking about most of the time',\n",
    "'Describe your health condition and your sleeping problems',\n",
    "'Do you know what’s behind your moods',\n",
    "'Describe your bad experience that you faced recently',\n",
    "'What are the main emotions you feeling lately',\n",
    "'What do you think when you are depressed',\n",
    "'Explain any history of mental illness in your family',\n",
    "'Describe your serious health illness you faced',\n",
    "'How will react if you loose your job',\n",
    "'What will you do to the person betrayed you',\n",
    "'Have you been thinking about killing yourself or hurting yourself in your way',\n",
    "'What do you think about suicide or killing someone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('jokes.txt','r',encoding=\"cp1252\")\n",
    "while(1):\n",
    "    full_file = file.readline()\n",
    "    split_file = full_file.split('/')\n",
    "\n",
    "    slashes = full_file.count('/')\n",
    "\n",
    "    line_of_joke = []\n",
    "    for i in range(slashes):\n",
    "        k=0\n",
    "    #    print(split_file[i])\n",
    "        commas = split_file[i].count('\"')\n",
    "#        print(commas)\n",
    "        length = int(commas/2)\n",
    "        if length == 0:\n",
    "            line_of_joke.append(split_file[i])\n",
    "        else:\n",
    "            for j in range(length):\n",
    "#            print(\"Here\")\n",
    "                line_of_joke.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
    "                if j==length-1:\n",
    "                    line_of_joke.append(split_file[i].split('\"')[k+2])\n",
    "                k=k+2\n",
    "#    break\n",
    "    for i in line_of_joke:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('jokes.txt','r',encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = []\n",
    "file = open('jokes.txt','r',encoding=\"cp1252\")\n",
    "x= file.readline()\n",
    "while(x):\n",
    "    x = file.readline()\n",
    "    split_file = x.split('/')\n",
    "    jokes.append(split_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
